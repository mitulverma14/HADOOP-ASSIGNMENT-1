--- Retriving data from StackExchange ---
/* Collecting 200000 records from stack Exchange having maximum View count. */
select * from Posts Where ViewCount > 100000 order by ViewCount desc;
select * from Posts where ViewCount > 58000 and ViewCount <= 100000 order by ViewCount desc;
select * from Posts where ViewCount > 42000 and ViewCount <= 58000 Order by ViewCount desc;
select * from Posts where ViewCount > 33000 and ViewCount <= 42000 Order by ViewCount desc;
select * from Posts where ViewCount > 31500 and ViewCount <= 33000 Order by ViewCount desc;

/* Data is collected in CSV file in five files. */

--- Load data into hadoop. ---

hadoop fs -put QueryResults.csv /Input

-- Loading the data into pig --

CS1 = LOAD '/Input/FirstQueryResults.csv' USING org.apache.pig.piggybank.storage.CSVExcelStorage(',', 'YES_MULTILINE','NOCHANGE','SKIP_INPUT_HEADER') AS (Id:int, PostTypeId:int,  AcceptedAnswerId:int, ParentId:int, CreationDate:chararray, DeletionDate:chararray, Score:int, ViewCount:int, Body:chararray, OwnerUserId:int, OwnerDisplayName:chararray, LastEditorUserId:int, LastEditorDisplayName:chararray, LastEditDate:chararray, LastActivityDate:chararray, Title:chararray, Tags:chararray, AnswerCount:int, CommentCount:int, FavoriteCount:int, ClosedDate:chararray, CommunityOwnedDate:chararray);
CS2 = LOAD '/Input/SecondQueryResults.csv' USING org.apache.pig.piggybank.storage.CSVExcelStorage(',', 'YES_MULTILINE','NOCHANGE','SKIP_INPUT_HEADER') AS (Id:int, PostTypeId:int,  AcceptedAnswerId:int, ParentId:int, CreationDate:chararray, DeletionDate:chararray, Score:int, ViewCount:int, Body:chararray, OwnerUserId:int, OwnerDisplayName:chararray, LastEditorUserId:int, LastEditorDisplayName:chararray, LastEditDate:chararray, LastActivityDate:chararray, Title:chararray, Tags:chararray, AnswerCount:int, CommentCount:int, FavoriteCount:int, ClosedDate:chararray, CommunityOwnedDate:chararray);
CS3 = LOAD '/Input/FourthQueryResults.csv' USING org.apache.pig.piggybank.storage.CSVExcelStorage(',', 'YES_MULTILINE','NOCHANGE','SKIP_INPUT_HEADER') AS (Id:int, PostTypeId:int,  AcceptedAnswerId:int, ParentId:int, CreationDate:chararray, DeletionDate:chararray, Score:int, ViewCount:int, Body:chararray, OwnerUserId:int, OwnerDisplayName:chararray, LastEditorUserId:int, LastEditorDisplayName:chararray, LastEditDate:chararray, LastActivityDate:chararray, Title:chararray, Tags:chararray, AnswerCount:int, CommentCount:int, FavoriteCount:int, ClosedDate:chararray, CommunityOwnedDate:chararray);
CS4 = LOAD '/Input/FirstQueryResults.csv' USING org.apache.pig.piggybank.storage.CSVExcelStorage(',', 'YES_MULTILINE','NOCHANGE','SKIP_INPUT_HEADER') AS (Id:int, PostTypeId:int,  AcceptedAnswerId:int, ParentId:int, CreationDate:chararray, DeletionDate:chararray, Score:int, ViewCount:int, Body:chararray, OwnerUserId:int, OwnerDisplayName:chararray, LastEditorUserId:int, LastEditorDisplayName:chararray, LastEditDate:chararray, LastActivityDate:chararray, Title:chararray, Tags:chararray, AnswerCount:int, CommentCount:int, FavoriteCount:int, ClosedDate:chararray, CommunityOwnedDate:chararray);
CS5 = LOAD '/Input/FirstQueryResults.csv' USING org.apache.pig.piggybank.storage.CSVExcelStorage(',', 'YES_MULTILINE','NOCHANGE','SKIP_INPUT_HEADER') AS (Id:int, PostTypeId:int,  AcceptedAnswerId:int, ParentId:int, CreationDate:chararray, DeletionDate:chararray, Score:int, ViewCount:int, Body:chararray, OwnerUserId:int, OwnerDisplayName:chararray, LastEditorUserId:int, LastEditorDisplayName:chararray, LastEditDate:chararray, LastActivityDate:chararray, Title:chararray, Tags:chararray, AnswerCount:int, CommentCount:int, FavoriteCount:int, ClosedDate:chararray, CommunityOwnedDate:chararray);

-- concat all the five CSV files
Final_CSV = UNION CS1, CS2, CS3, CS4, CS5;

--Consider only the required columns--
/*Remove commas from 'Body','Title' and 'Tags' fields. Follow the below command for all the genrated file above*/
NC_File = FOREACH Final_CSV GENERATE  Id AS Id, Score AS Score, REPLACE(Body,',*','') AS Body, OwnerUserId AS OwnerUserId, REPLACE(Title,',*','') AS Title, REPLACE(Tags,',*','') AS Tags;

/* Remove '/n' from 'Body','Title' and 'Tags' fields. */
NL_File = FOREACH NC_File GENERATE Id AS Id, Score AS Score, REPLACE(Body,'\n*','') AS Body, OwnerUserId AS OwnerUserId, REPLACE(Title,'\n*','') AS Title, REPLACE(Tags,'\n*','') AS Tags;

/* Remove <.*?> from 'Body','Title' and 'Tags' fields. */
Clean_File = FOREACH NL_File GENERATE Id AS Id, Score AS Score, REPLACE(Body,'<.*?>',' ') AS Body, OwnerUserId AS OwnerUserId, REPLACE(Title,'<.*?>',' ') AS Title, Tags AS Tags;

-- Eliminate rows with NULL values in 'OwnerUserId' and 'Score' fields.
ValidFile = FILTER Clean_File BY (OwnerUserId IS NOT NULL) AND (Score IS NOT NULL);

-- Selecting 200000 and stroing in new relation StackExchange.
StackExchange = limit ValidFile 200000;

-- Storing top 200000 records from the Cleaned validcsv file.
STORE StackExchange INTO '/output' USING org.apache.pig.piggybank.storage.CSVExcelStorage(',');


-- Manupulating data with hive --
set hive.cli.print.header=true

create table posts(Id int, Score int, Body String, OwnerUserId Int, Title String, Tags String) 
row format delimited 
FIELDS TERMINATED BY ','
location '/Mitul';

/* The top 10 posts by score */
select id, score, title, body, tags from posts order by score desc limit 10;

/* The top 10 users by post score */
select owneruserid, sum(score) as Total_Score from posts group by owneruserid order by TS desc limit 10;

/* The number of distinct users, who used the word “Hadoop” in one of their posts */
select count(distinct owneruserid) from posts where body like '%hadoop%' OR title like '%hadoop%' OR Tags like %hadoop%;

--- TF-IDF ---
-- Store top users in a separate table called Users:

CREATE TABLE Users
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',' AS
SELECT OwnerUserId, SUM(Score) AS TotalScore
FROM posts
GROUP BY OwnerUserId
ORDER BY TotalScore DESC LIMIT 10;

-- collect body and title of the users in TopUsers and store it in a separate table called TopPosts

CREATE TABLE TopPosts AS
SELECT OwnerUserId, Body, Title
FROM posts
WHERE OwnerUserId in (SELECT OwnerUserId FROM TopUsers)
GROUP BY OwnerUserId, Body, Title, Tags;

-- Store the TopPosts results into an HDFS directory

INSERT OVERWRITE DIRECTORY 'hiveResults'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
SELECT OwnerUserId, Body, Title
FROM TopUserPosts
GROUP BY OwnerUserId, Body, Title;

-- Download the file to the Local system -- 
folderPath = "C:\\Users\\Kanishk Verma\\Downloads\\";

hadoop jar /home/mitul_verma3/hadoop-streaming-2.7.1.jar -file 
/home/mitul_verma3/MapperPhaseOne.py /home/mitul_verma3/ReducerPhaseOne.py 
-mapper "python MapperPhaseOne.py" -reducer "python ReducerPhaseOne.py" -input /Mitul -output /output

hadoop jar /home/mitul_verma3/hadoop-streaming-2.7.1.jar -file 
/home/mitul_verma3/MapperPhaseTwo.py /home/mitul_verma3/ReducerPhaseTwo.py 
-mapper "python MapperPhaseTwo.py" -reducer "python ReducerPhasePTwo.py" -input /MP1 -output /output1

hadoop jar /home/mitul_verma3/hadoop-streaming-2.7.1.jar -file 
/home/mitul_verma3/MapperPhaseThree.py /home/mitul_verma3/ReducerPhaseThree.py 
-mapper "python MapperPhaseThree.py" -reducer "python ReducerPhasePThree.py" -input /MP2 -output /output2

-- Combining the output from output2 directory
hadoop fs -cat /output2/part-0* > Final_Results.csv


-- Create table in hive and load the csv into the table. --
sed -e 's/\s/,/g' Final_Results.csv > out1.csv


create table tfidf(Term String, Id int, Tfidf float) 
row format delimited 
FIELDS TERMINATED BY ',';


load data local inpath 'out1.csv' overwrite into table tfidf;

-- Run the following query to get the top 10 terms for each of the top 10 users from Query 3.II. --
load data local inpath 'out1.csv' overwrite into table tfidf;

select * 
from (
select row_number()
over(partition by id
order by tfidf desc) as tfidfrank, * from tfidf) n
where tfidfrank in (1,2,3,4,5,6,7,8,9,10);


The mapreduce programs are from https://github.com/SatishUC15/TFIDF-HadoopMapReduce#tfidf-hadoop 
with minor changes (i.e. addition of stop words and the related if condition in MapperPhaseOne.py)